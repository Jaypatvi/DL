#2)
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

def load_data():
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    X_train, X_test = X_train / 255.0, X_test / 255.0  
    return (X_train, y_train), (X_test, y_test)

(X_train, y_train), (X_test, y_test) = load_data()
def create_lenet():
    model = models.Sequential([
        layers.Conv2D(6, (5, 5), activation='tanh', input_shape=(32, 32, 3)),
        layers.AveragePooling2D(pool_size=2),
        layers.Conv2D(16, (5, 5), activation='tanh'),
        layers.AveragePooling2D(pool_size=2),
        layers.Flatten(),
        layers.Dense(120, activation='tanh'),
        layers.Dense(84, activation='tanh'),
        layers.Dense(10, activation='softmax')
    ])
    return model
def create_alexnet():
    model = models.Sequential([
        layers.Conv2D(64, (3, 3), strides=1, activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),

        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),

        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),

        layers.Flatten(),
        layers.Dense(1024, activation='relu'),
        layers.Dense(512, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

def create_inception_module(input_layer, filters):
    branch1 = layers.Conv2D(filters[0], (1, 1), activation='relu')(input_layer)

    branch2 = layers.Conv2D(filters[1], (1, 1), activation='relu')(input_layer)
    branch2 = layers.Conv2D(filters[2], (3, 3), padding='same', activation='relu')(branch2)

    branch3 = layers.Conv2D(filters[3], (1, 1), activation='relu')(input_layer)
    branch3 = layers.Conv2D(filters[4], (5, 5), padding='same', activation='relu')(branch3)

    branch4 = layers.MaxPooling2D((3, 3), strides=1, padding='same')(input_layer)
    branch4 = layers.Conv2D(filters[5], (1, 1), activation='relu')(branch4)

    output_layer = layers.concatenate([branch1, branch2, branch3, branch4], axis=-1)
    return output_layer

def create_googlenet():
    input_layer = layers.Input(shape=(32, 32, 3))
    x = create_inception_module(input_layer, [64, 96, 128, 16, 32, 32])
    x = layers.Flatten()(x)
    x = layers.Dense(10, activation='softmax')(x)
    model = models.Model(inputs=input_layer, outputs=x)
    return model
def train_and_evaluate(model, X_train, y_train, X_test, y_test, optimizer_name):
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train, y_train, epochs=2, batch_size=64, validation_data=(X_test, y_test), verbose=1)
    return history

lenet = create_lenet()
history_lenet = train_and_evaluate(lenet, X_train, y_train, X_test, y_test, 'LeNet')

alexnet = create_alexnet()
history_alexnet = train_and_evaluate(alexnet, X_train, y_train, X_test, y_test, 'AlexNet')

googlenet = create_googlenet()
history_googlenet = train_and_evaluate(googlenet, X_train, y_train, X_test, y_test, 'GoogLeNet')
def plot_comparison(histories, labels):
    plt.figure(figsize=(10, 5))
    for history, label in zip(histories, labels):
        plt.plot(history.history['val_accuracy'], label=f'{label} Val Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Validation Accuracy')
    plt.title('Model Comparison')
    plt.legend()
    plt.show()

plot_comparison([history_lenet, history_alexnet, history_googlenet], ['LeNet', 'AlexNet', 'GoogLeNet'])
